{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cellular-celebrity",
   "metadata": {},
   "source": [
    "# Pre-processing Notebook\n",
    "- Input for this notebook: SCSE faculty staff DBLP info XML as input\n",
    "- Task: Select correct network representation\n",
    "- Output: process and create faculty network for analysis (assuming dataflow methodology, a network should be returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "breathing-edition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "attempted-employee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import faculty details into df\n",
    "faculty_df = pd.read_excel('Faculty.xlsx')\n",
    "\n",
    "# Select relevant columns\n",
    "faculty_df = faculty_df[['Faculty', 'Position', 'Gender', 'Management', 'DBLP', 'Area']]\n",
    "\n",
    "# Create dictionary\n",
    "faculty_dict = faculty_df.to_dict('index')\n",
    "\n",
    "# Declare empty List\n",
    "faculty_list = []\n",
    "\n",
    "'''\n",
    "# each[1] corresponds to lumped meta data\n",
    "# each[1]['Faculty'] corresonds to 'Faculty'\n",
    "# each[1]['Position'] corresonds to 'Position'\n",
    "# each[1]['Gender'] corresonds to 'Gender'\n",
    "# each[1]['Management'] corresponds to 'Management'\n",
    "# each[1]['DBLP'] corresponds to 'DBLP'\n",
    "# each[1]['Area'] corresponds to 'Area'\n",
    "'''\n",
    "# Iterate over faculty_dict to fill up faculty_list\n",
    "for each in faculty_dict.items():\n",
    "    \n",
    "    node_no = each[0] \n",
    "    faculty = each[1]['Faculty']\n",
    "    position = each[1]['Position']\n",
    "    gender = each[1]['Gender']\n",
    "    management = each[1]['Management']\n",
    "    dblp = each[1]['DBLP']\n",
    "    area = each[1]['Area']\n",
    "    \n",
    "    faculty_list.append((node_no, {'Faculty': faculty}))\n",
    "    faculty_list.append((node_no, {'Position': position}))\n",
    "    faculty_list.append((node_no, {'Gender': gender}))\n",
    "    faculty_list.append((node_no, {'Management': management}))\n",
    "    faculty_list.append((node_no, {'DBLP': dblp}))\n",
    "    faculty_list.append((node_no, {'Area': area}))\n",
    "    \n",
    "# Declare empty new graph for faculty network\n",
    "faculty_graph = nx.MultiGraph()\n",
    "\n",
    "# Fill up empty graph w/ faculty_list\n",
    "faculty_graph.add_nodes_from(faculty_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-effect",
   "metadata": {},
   "outputs": [],
   "source": [
    "faculty_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-custody",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"empty_faculty_graph.pkl\"\n",
    "\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(faculty_graph, open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-atlanta",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(faculty_graph.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-japanese",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to scrape correspondly DBL data, then augment them with existing dataset\n",
    "# have to then decide how we'll be using augment faculty details to connect and generate the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-infection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment to use previously-extracted DBLP raw data to map collaborations between faculty staff\n",
    "\n",
    "# Import dblp_df.csv as DF\n",
    "dblp_df = pd.read_csv('dblp_df.csv')\n",
    "\n",
    "# Create categorical class counts for each article\n",
    "\n",
    "# Use categorical class counts to map all related faculty members with a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "dblp_multi_df = dblp_df[dblp_df['key'].duplicated(keep=False)]\n",
    "dblp_multi_df.sort_values(by=['key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-wednesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get categorical uniques in dataframe\n",
    "categorical_list = dblp_multi_df['key'].drop_duplicates().tolist()\n",
    "\n",
    "# create list to store key:f_index_list mappins\n",
    "key_findex_list = []\n",
    "\n",
    "# use categorical uniques to return df records\n",
    "for each in categorical_list:\n",
    "    # extract f_index values from returned df records into a list\n",
    "    mappings = dblp_multi_df[dblp_multi_df['key'] == each]['f_index'].tolist()\n",
    "    # create unique pair-wise combinations for mappings (needed for networkx) \n",
    "    mappings_pair = list(combinations(mappings, 2)) \n",
    "    year = dblp_multi_df[dblp_multi_df['key'] == each]['Year'].iloc[0]\n",
    "    key_findex_list.append([mappings_pair, each, year])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-marina",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairings = np.array(key_findex_list)[:,[0,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-matter",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_paired_edges = [x for x in pairings if x[0][0][0] != x[0][0][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-steam",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_paired_edges = [[list(set(x[0])), x[1]] for x in distinct_paired_edges]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-motor",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_paired_edges = [[[y, x[1]]for y in x[0]] for x in distinct_paired_edges]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the flipped ones i.e I only want (0,16) and not (16,0)\n",
    "distinct_paired_edges = [[y for y in x if y[0][0] < y[0][1]] for x in distinct_paired_edges]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-american",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_paired_edges = [[x[0][0], x[0][0], x[0][1]] for x in distinct_paired_edges]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_year_pairings = [[x[0][0], x[0][1], x[1]] for x in distinct_paired_edges]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-controversy",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_year_pairings = [tuple(x) for x in edge_year_pairings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-hypothesis",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-stereo",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-jesus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save key_findex_list into pickle file for easy replication\n",
    "\n",
    "file_name = \"edge_year_pairings.pkl\"\n",
    "\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(edge_year_pairings, open_file)\n",
    "open_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-strength",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over key_findex_list to populate initial network\n",
    "\n",
    "for edge in edge_year_pairings:\n",
    "    faculty_graph.add_edge(edge[0], edge[1], label = edge[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save faculty_graph into pickle file for easy replication\n",
    "\n",
    "file_name = \"faculty_graph.pkl\"\n",
    "\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(faculty_graph, open_file)\n",
    "open_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-chapel",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def save_plot_faculty_graph():\n",
    "\n",
    "    # Set figure for graph\n",
    "    plt.figure(figsize=(35, 20))\n",
    "\n",
    "    # Draw the graph\n",
    "    nx.draw(faculty_graph, with_labels=True, font_size=10,\n",
    "            node_color='red', font_color='white', edge_color='grey', node_size=250)\n",
    "\n",
    "    # Save the graph\n",
    "    plt.savefig(\"collab_graph.png\", dpi=326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-pantyhose",
   "metadata": {},
   "outputs": [],
   "source": [
    "faculty_graph.edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-japan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove isolates\n",
    "faculty_graph.remove_nodes_from(list(nx.isolates(faculty_graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-extent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set figure for graph\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Draw the graph (with isolates removed)\n",
    "nx.draw(faculty_graph, with_labels=True, font_size=10,\n",
    "        node_color='red', font_color='white', edge_color='grey', node_size=300)\n",
    "\n",
    "# Save the graph\n",
    "plt.savefig(\"collab_graph_connected_only.png\", dpi=326)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-creek",
   "metadata": {},
   "source": [
    "# Sourav & Frens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "varying-prospect",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "wrapped-projection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import faculty details into df\n",
    "faculty_df = pd.read_excel('Faculty.xlsx')\n",
    "\n",
    "# Select relevant columns\n",
    "faculty_df = faculty_df[['Faculty', 'Position', 'Gender', 'Management', 'DBLP', 'Area']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "gothic-discharge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create another networkx graph with existing SCSE graph + 1000 apostles\n",
    "\n",
    "# Retrieve 1000 apostles\n",
    "with open('1000_faculty.pkl', 'rb') as f:\n",
    "    thousand_apostles_list = pickle.load(f)\n",
    "    \n",
    "# Augment Previously created faculty_df w/ 1000 apostles\n",
    "for new_faculty in thousand_apostles_list:\n",
    "    row = pd.Series([new_faculty, '-', '-', '-', '-', '-'], index=faculty_df.columns)\n",
    "    faculty_df = faculty_df.append(row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "prerequisite-prison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary\n",
    "faculty_dict = faculty_df.to_dict('index')\n",
    "\n",
    "# Declare empty List\n",
    "faculty_list = []\n",
    "\n",
    "'''\n",
    "# each[1] corresponds to lumped meta data\n",
    "# each[1]['Faculty'] corresonds to 'Faculty'\n",
    "# each[1]['Position'] corresonds to 'Position'\n",
    "# each[1]['Gender'] corresonds to 'Gender'\n",
    "# each[1]['Management'] corresponds to 'Management'\n",
    "# each[1]['DBLP'] corresponds to 'DBLP'\n",
    "# each[1]['Area'] corresponds to 'Area'\n",
    "'''\n",
    "# Iterate over faculty_dict to fill up faculty_list\n",
    "for each in faculty_dict.items():\n",
    "    \n",
    "    node_no = each[0] \n",
    "    faculty = each[1]['Faculty']\n",
    "    position = each[1]['Position']\n",
    "    gender = each[1]['Gender']\n",
    "    management = each[1]['Management']\n",
    "    dblp = each[1]['DBLP']\n",
    "    area = each[1]['Area']\n",
    "    \n",
    "    faculty_list.append((node_no, {'Faculty': faculty}))\n",
    "    faculty_list.append((node_no, {'Position': position}))\n",
    "    faculty_list.append((node_no, {'Gender': gender}))\n",
    "    faculty_list.append((node_no, {'Management': management}))\n",
    "    faculty_list.append((node_no, {'DBLP': dblp}))\n",
    "    faculty_list.append((node_no, {'Area': area}))\n",
    "    \n",
    "# Declare empty new graph for faculty network\n",
    "faculty_graph_1k = nx.MultiGraph()\n",
    "\n",
    "# Fill up empty graph w/ faculty_list\n",
    "faculty_graph_1k.add_nodes_from(faculty_list)\n",
    "\n",
    "# Assign f_index to all augmented faculty members list\n",
    "faculty_df.insert(0, 'f_index', faculty_df.index.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "specified-preference",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment to use previously-extracted DBLP raw data to map collaborations between faculty staff and 1k apostles\n",
    "\n",
    "# Import dblp_df_2.csv as DF\n",
    "with open('dblp_12k_processed_df.pkl', 'rb') as f:\n",
    "    dblp_df = pickle.load(f)\n",
    "\n",
    "unique_oa_list = dblp_df['Other Authors'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "technical-passenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of lists with all authors:\n",
    "f_list = []\n",
    "oa_list = []\n",
    "for i, row in dblp_df.iterrows():\n",
    "    f_list.append(row['Faculty'])\n",
    "    oa_list.append(row['Other Authors'])\n",
    "    \n",
    "# create the combined list\n",
    "combined_list = []\n",
    "for i in range(len(f_list)):\n",
    "    if f_list[i] not in oa_list[i]:\n",
    "        temp_list = oa_list[i]\n",
    "        temp_list.append(f_list[i])\n",
    "        combined_list.append(temp_list)\n",
    "        \n",
    "# filter out duplicate lists in list of lists        \n",
    "seen = set()\n",
    "uc_list = [x for x in combined_list if frozenset(x) not in seen and not seen.add(frozenset(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "exotic-laugh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmented Faculty f_index dict:\n",
    "af_dict = dict(zip(faculty_df['Faculty'], faculty_df['f_index']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "essential-breakfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of edges from uc_list\n",
    "\n",
    "uc_edges_list = []\n",
    "for each in uc_list:\n",
    "    uc_temp_list = []\n",
    "    for each_2 in each:\n",
    "        try:\n",
    "            uc_temp_list.append(af_dict[each_2])\n",
    "        except:\n",
    "            continue\n",
    "    uc_edges_list.append(uc_temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fourth-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out duplicate lists in list of lists again         \n",
    "seen = set()\n",
    "uuc_edges_list = [x for x in uc_edges_list if frozenset(x) not in seen and not seen.add(frozenset(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "correct-failure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation of list of edges (faculty + 1k apostles) for graphx\n",
    "faculty_1000_list = []\n",
    "for each in uuc_edges_list:\n",
    "    pairs = list(combinations(each, 2))\n",
    "    faculty_1000_list.append(pairs)\n",
    "\n",
    "# flatten list\n",
    "flat_edge_list = []\n",
    "for each in faculty_1000_list:\n",
    "    for each_2 in each:\n",
    "        flat_edge_list.append(each_2)\n",
    "    \n",
    "# retrieve only unique edge pairings\n",
    "unique_flat_edge_list = list(set(flat_edge_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "buried-lafayette",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add edges to faculty_1k Graph\n",
    "faculty_graph_1k.add_edges_from(unique_flat_edge_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-throat",
   "metadata": {},
   "source": [
    "# Note: Even when extracting values from a DF into a list, any updates to the list will cascade changes back into the source DF the values came from. I'm not sure why there's this pointer issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-region",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set figure for graph\n",
    "plt.figure(figsize=(200, 100))\n",
    "\n",
    "# Draw the graph (with isolates removed)\n",
    "nx.spring_layout(faculty_graph_1k, k=0.25, iterations=20)\n",
    "nx.draw(faculty_graph_1k, with_labels=True, font_size=20,\n",
    "        node_color='red', font_color='white', edge_color='grey', node_size=1500)\n",
    "\n",
    "# Save the graph\n",
    "plt.savefig(\"faculty_graph_1k.png\", dpi=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "developed-black",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store 1085 faculty df and nx graph as 2 respective pickles\n",
    "\n",
    "faculty_df.to_csv('faculty_df.csv', index=False)  \n",
    "\n",
    "with open('faculty_1k_df.pkl', 'wb') as f:\n",
    "    pickle.dump(faculty_df, f)\n",
    "\n",
    "with open('faculty_1k_graph.pkl', 'wb') as f:\n",
    "    pickle.dump(faculty_graph_1k, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "recorded-continent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_index</th>\n",
       "      <th>Faculty</th>\n",
       "      <th>Position</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Management</th>\n",
       "      <th>DBLP</th>\n",
       "      <th>Area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A S Madhukumar</td>\n",
       "      <td>Associate Professor</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>https://dblp.uni-trier.de/pers/m/Madhukumar:A=...</td>\n",
       "      <td>Computer Networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Alexei Sourin</td>\n",
       "      <td>Associate Professor</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>https://dblp.org/pers/s/Sourin:Alexei.html</td>\n",
       "      <td>Computer Graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Anupam Chattopadhyay</td>\n",
       "      <td>Associate Professor</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>https://dblp.org/pers/c/Chattopadhyay:Anupam.html</td>\n",
       "      <td>Computer Architecture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Anwitaman Datta</td>\n",
       "      <td>Associate Professor</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>https://dblp.org/pers/d/Datta:Anwitaman.html</td>\n",
       "      <td>Distributed Systems</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Arijit Khan</td>\n",
       "      <td>Assistant Professor</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>https://dblp.org/pers/k/Khan:Arijit.html</td>\n",
       "      <td>Data Management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1080</th>\n",
       "      <td>1080</td>\n",
       "      <td>Amitabha Das</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081</th>\n",
       "      <td>1081</td>\n",
       "      <td>Herty Liany</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082</th>\n",
       "      <td>1082</td>\n",
       "      <td>Whye Loon Tung</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1083</th>\n",
       "      <td>1083</td>\n",
       "      <td>Virgil D. Gligor</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084</th>\n",
       "      <td>1084</td>\n",
       "      <td>Linlin Liu</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1085 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      f_index               Faculty             Position Gender Management  \\\n",
       "0           0        A S Madhukumar  Associate Professor      M          N   \n",
       "1           1         Alexei Sourin  Associate Professor      M          N   \n",
       "2           2  Anupam Chattopadhyay  Associate Professor      M          N   \n",
       "3           3       Anwitaman Datta  Associate Professor      M          N   \n",
       "4           4           Arijit Khan  Assistant Professor      M          N   \n",
       "...       ...                   ...                  ...    ...        ...   \n",
       "1080     1080          Amitabha Das                    -      -          -   \n",
       "1081     1081           Herty Liany                    -      -          -   \n",
       "1082     1082        Whye Loon Tung                    -      -          -   \n",
       "1083     1083      Virgil D. Gligor                    -      -          -   \n",
       "1084     1084            Linlin Liu                    -      -          -   \n",
       "\n",
       "                                                   DBLP                   Area  \n",
       "0     https://dblp.uni-trier.de/pers/m/Madhukumar:A=...      Computer Networks  \n",
       "1            https://dblp.org/pers/s/Sourin:Alexei.html      Computer Graphics  \n",
       "2     https://dblp.org/pers/c/Chattopadhyay:Anupam.html  Computer Architecture  \n",
       "3          https://dblp.org/pers/d/Datta:Anwitaman.html    Distributed Systems  \n",
       "4              https://dblp.org/pers/k/Khan:Arijit.html        Data Management  \n",
       "...                                                 ...                    ...  \n",
       "1080                                                  -                      -  \n",
       "1081                                                  -                      -  \n",
       "1082                                                  -                      -  \n",
       "1083                                                  -                      -  \n",
       "1084                                                  -                      -  \n",
       "\n",
       "[1085 rows x 7 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faculty_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-employee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
