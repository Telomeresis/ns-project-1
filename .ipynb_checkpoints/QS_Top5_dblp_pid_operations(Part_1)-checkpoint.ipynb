{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "million-gospel",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "The purpose of this script is to use the previously extracted faculty info and retrieve all related publication info from dblp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sapphire-albuquerque",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pickle\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "connected-regression",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Retrieve pickle-ricked data\n",
    "with open('top_5_list.pkl', 'rb') as f:\n",
    "    top_5_list = pickle.load(f) # [[retrieve_mit_list()], [retrieve_stanford_list()], [retrieve_cmu_list()], [retrieve_nus_list()], [retrieve_ucb_list()]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-produce",
   "metadata": {},
   "source": [
    "# Step 2: Create DF from previously made lists\n",
    "assume that univesity school sub-department doesn't matter for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alternate-nicaragua",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine existing list; assuming sub-fac doesn't matter for now\n",
    "mit_list = top_5_list[0][0][0] + top_5_list[0][0][1] + top_5_list[0][0][2] \n",
    "stanford_list = top_5_list[1][0][0] + top_5_list[1][0][1]\n",
    "cmu_list = top_5_list[2][0] # singular list\n",
    "nus_list = top_5_list[3][0][0] + top_5_list[3][0][1] + top_5_list[3][0][2] + top_5_list[3][0][3]\n",
    "ucb_list = top_5_list[4][0][0] + top_5_list[4][0][1] \n",
    "\n",
    "# Append university meta data for DF ease of use\n",
    "mit_append_list = ['MIT'] * len(mit_list)\n",
    "stanford_append_list = ['Stanford'] * len(stanford_list)\n",
    "cmu_append_list = ['CMU'] * len(cmu_list)\n",
    "nus_append_list = ['NUS'] * len(nus_list)\n",
    "ucb_append_list = ['UCB'] * len(ucb_list)\n",
    "\n",
    "qs5_df = pd.DataFrame()\n",
    "qs5_df['University'] = mit_append_list + stanford_append_list + cmu_append_list + nus_append_list + ucb_append_list\n",
    "qs5_df['Faculty'] = mit_list + stanford_list + cmu_list + nus_list + ucb_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-nebraska",
   "metadata": {},
   "source": [
    "# [Skip to Step 7] for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-level",
   "metadata": {},
   "source": [
    "# Step 3: Search query list pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-wells",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty list for search query automation\n",
    "q_list = []\n",
    "\n",
    "# Iterate over extracted faculty names, convert spaces to %20, then append to search query list\n",
    "for each in qs5_df['Faculty']:\n",
    "    query = str(each).replace(\" \", \"%20\") # need to convert to string??\n",
    "    q_list.append(query) #  this creates a list with 983 queries lmao"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-nerve",
   "metadata": {},
   "source": [
    "# Step 4: Use processed query list to retrieve dblp raw html and store into list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-kruger",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "url=\"https://dblp.org/search?q=\"\n",
    "\n",
    "# Declare list to store extracted content\n",
    "qs5_content_list = []\n",
    "\n",
    "i = 0\n",
    "# Iterate using q_list to make a GET request to fetch raw HTML content\n",
    "for each in q_list:\n",
    "    html_content = requests.get(url+each).text\n",
    "    qs5_content_list.append(html_content)\n",
    "    i+=1\n",
    "    if (i % 10 == 0):\n",
    "        print(i)\n",
    "    \n",
    "# Store content_list with pickle\n",
    "with open('qs5_content_list.pkl', 'wb') as f:\n",
    "    pickle.dump(qs5_content_list, f)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-access",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve content_list with pickle\n",
    "with open('qs5_content_list.pkl', 'rb') as f:\n",
    "    qs5_content_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-style",
   "metadata": {},
   "source": [
    "# Step 5: Process each item in the qs5_content list w/ BS4 (983 times lmao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-ethnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare empty list for storing soups\n",
    "qs5_soup_list = []\n",
    "\n",
    "i = 0\n",
    "for each in qs5_content_list:\n",
    "    soup = BeautifulSoup(each, \"lxml\")\n",
    "    qs5_soup_list.append(soup.prettify())\n",
    "    i+=1\n",
    "    if (i % 100 == 0):\n",
    "        print(i)\n",
    "\n",
    "# Store pretty_soup_list with pickle\n",
    "with open('qs5_soup_list.pkl', 'wb') as f:\n",
    "    pickle.dump(qs5_soup_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-milwaukee",
   "metadata": {},
   "source": [
    "# Step 6: PID Extraction Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-formation",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Declare empty pid list for storing pids\n",
    "qs5_pid_list = []\n",
    "\n",
    "i = 0\n",
    "# Iterate over pretty_soup_list to extract pid\n",
    "for each in qs5_soup_list:\n",
    "    converted_each = BeautifulSoup(each, \"html.parser\") # need to convert lmao\n",
    "    pid = converted_each.find(\"a\", itemprop=\"url\")\n",
    "    try:\n",
    "        pid_list.append(pid['href']) # select href tag to get pid url\n",
    "    except:\n",
    "        pid_list.append('ERROR')\n",
    "    i+=1\n",
    "    if (i % 100 == 0):\n",
    "        print(i)\n",
    "        \n",
    "# Store qs5_pid_list with pickle\n",
    "with open('qs5_pid_list.pkl', 'wb') as f:\n",
    "    pickle.dump(qs5_pid_list, f)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-confirmation",
   "metadata": {},
   "source": [
    "# Step 7: Append PID values into DF + duplicate removal + manual re-search for inaccurate names + PID source (Automated/Manual) + removal of individuals w/out publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "infinite-symphony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve qs5_pid_list with pickle\n",
    "with open('qs5_pid_list.pkl', 'rb') as f:\n",
    "    qs5_pid_list = pickle.load(f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "recent-course",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append PID list to df column\n",
    "qs5_df['PID'] = qs5_pid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "impressive-category",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discovered later: duplicate records. Step to remove duplicates.\n",
    "qs5_df = qs5_df.astype(str).drop_duplicates() # Before pruning, len=983. After pruning, len=666 >:D\n",
    "qs5_df = qs5_df.reset_index(drop=True) # reset index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "overall-seventh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70 records with non-existent dfs\n",
    "# qs5_df['Faculty'].loc[qs5_df['PID'] == 'ERROR'].to_excel(\"Error_Manual.xlsx\")\n",
    "replacement_df = pd.read_excel(\"Error_Manual.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "involved-childhood",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create new column to indicate augmentation status\n",
    "qs5_df['PID Source'] = 'Automated'\n",
    "\n",
    "# Fill up manually-obtained PID info\n",
    "for index, row in qs5_df.iterrows():\n",
    "    # row[1] = qs5_df['Faculty'] \n",
    "    for index_2, row_2 in replacement_df.iterrows(): # sorry, I know this is terrible time-space complexity    \n",
    "        if (row[1] == replacement_df['Faculty'][index_2]):\n",
    "            # row[2] = qs5_df['PID']\n",
    "            row[2] = replacement_df['PID'][index_2]\n",
    "            # row[2] = qs5_df['PID Source']\n",
    "            row[3] = 'Manual'\n",
    "            \n",
    "# Remove rows where PID data is unavailable, usually due to indiviudals having no publications\n",
    "qs5_df = qs5_df[(qs5_df['PID'] != 'UNAVAILABLE')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "higher-childhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store qs5_df\n",
    "with open('qs5_step7_df.pkl', 'wb') as f:\n",
    "    pickle.dump(qs5_df, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
